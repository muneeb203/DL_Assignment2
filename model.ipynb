{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal Clause Semantic Similarity - Model Training and Evaluation\n",
        "\n",
        "This notebook implements and compares two models for legal clause semantic similarity:\n",
        "1. Siamese BiLSTM\n",
        "2. Attention-based Encoder\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Set style for plots\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except:\n",
        "        plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading via preprocessing.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import preprocessing functions\n",
        "from preprocessing import load_data, create_pairs, prepare_tokenizer_and_sequences, split_data\n",
        "\n",
        "# Load and preprocess data\n",
        "print(\"Loading data...\")\n",
        "df = load_data('archive')\n",
        "\n",
        "# Create pairs\n",
        "print(\"\\nCreating pairs...\")\n",
        "pairs_df = create_pairs(df, num_positive_pairs=50000, num_negative_pairs=50000)\n",
        "\n",
        "# Prepare tokenizer and sequences\n",
        "print(\"\\nPreparing tokenizer and sequences...\")\n",
        "tokenizer, X1, X2, vocab_size = prepare_tokenizer_and_sequences(\n",
        "    pairs_df, num_words=20000, maxlen=120\n",
        ")\n",
        "\n",
        "# Get labels\n",
        "y = pairs_df['label'].values\n",
        "\n",
        "# Split data\n",
        "X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test, test_indices = split_data(\n",
        "    X1, X2, y, test_size=0.15, val_size=0.15\n",
        ")\n",
        "\n",
        "print(\"\\nData shapes:\")\n",
        "print(f\"X1_train: {X1_train.shape}, X2_train: {X2_train.shape}\")\n",
        "print(f\"X1_val: {X1_val.shape}, X2_val: {X2_val.shape}\")\n",
        "print(f\"X1_test: {X1_test.shape}, X2_test: {X2_test.shape}\")\n",
        "print(f\"y_train: {y_train.shape}, y_val: {y_val.shape}, y_test: {y_test.shape}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Test indices shape: {test_indices.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model 1: Siamese BiLSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_siamese_bilstm(vocab_size, embedding_dim=128, lstm_units=128, maxlen=120):\n",
        "    \"\"\"\n",
        "    Build Siamese BiLSTM model for semantic similarity.\n",
        "    \n",
        "    Architecture:\n",
        "    - Shared Embedding layer\n",
        "    - Shared Bidirectional LSTM\n",
        "    - Combine using [u, v, |u-v|, u*v]\n",
        "    - Dense layers with dropout\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    input1 = layers.Input(shape=(maxlen,), name='input1')\n",
        "    input2 = layers.Input(shape=(maxlen,), name='input2')\n",
        "    \n",
        "    # Shared embedding layer\n",
        "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True, name='embedding')\n",
        "    \n",
        "    # Shared BiLSTM layer\n",
        "    bilstm = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False), name='bilstm')\n",
        "    \n",
        "    # Process both inputs through shared layers\n",
        "    embedded1 = embedding(input1)\n",
        "    embedded2 = embedding(input2)\n",
        "    \n",
        "    # Apply BiLSTM\n",
        "    lstm_out1 = bilstm(embedded1)\n",
        "    lstm_out2 = bilstm(embedded2)\n",
        "    \n",
        "    # Combine vectors: [u, v, |u-v|, u*v]\n",
        "    diff = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([lstm_out1, lstm_out2])\n",
        "    multiply = layers.Lambda(lambda x: x[0] * x[1])([lstm_out1, lstm_out2])\n",
        "    concat = layers.Concatenate()([lstm_out1, lstm_out2, diff, multiply])\n",
        "    \n",
        "    # Dense layers\n",
        "    dense1 = layers.Dense(128, activation='relu', name='dense1')(concat)\n",
        "    dropout1 = layers.Dropout(0.3, name='dropout1')(dense1)\n",
        "    dense2 = layers.Dense(64, activation='relu', name='dense2')(dropout1)\n",
        "    output = layers.Dense(1, activation='sigmoid', name='output')(dense2)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs=[input1, input2], outputs=output, name='Siamese_BiLSTM')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "print(\"Building Siamese BiLSTM model...\")\n",
        "bilstm_model = build_siamese_bilstm(vocab_size, embedding_dim=128, lstm_units=128, maxlen=120)\n",
        "\n",
        "# Compile model\n",
        "bilstm_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "bilstm_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callbacks for training\n",
        "bilstm_callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint('bilstm_model.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n",
        "]\n",
        "\n",
        "# Train model\n",
        "print(\"Training Siamese BiLSTM model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "bilstm_history = bilstm_model.fit(\n",
        "    [X1_train, X2_train], y_train,\n",
        "    validation_data=([X1_val, X2_val], y_val),\n",
        "    batch_size=64,\n",
        "    epochs=25,\n",
        "    callbacks=bilstm_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "bilstm_training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {bilstm_training_time:.2f} seconds\")\n",
        "print(\"Best model weights restored automatically by EarlyStopping\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model 2: Attention-based Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def attention_layer(inputs, attention_dim=128):\n",
        "    \"\"\"\n",
        "    Self-attention layer for encoding sequences.\n",
        "    \"\"\"\n",
        "    # Dense layer for attention scoring\n",
        "    attention_scores = layers.Dense(attention_dim, activation='tanh', name='attention_dense1')(inputs)\n",
        "    attention_scores = layers.Dense(1, activation=None, name='attention_dense2')(attention_scores)\n",
        "    \n",
        "    # Remove the last dimension (squeeze from (batch, seq, 1) to (batch, seq))\n",
        "    attention_scores = layers.Lambda(lambda x: tf.squeeze(x, axis=-1), name='attention_squeeze')(attention_scores)\n",
        "    \n",
        "    # Apply softmax along sequence dimension (axis=1)\n",
        "    attention_weights = layers.Softmax(axis=1, name='attention_softmax')(attention_scores)\n",
        "    \n",
        "    # Expand dimensions for broadcasting: (batch, seq) -> (batch, seq, 1)\n",
        "    attention_weights = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), name='attention_expand')(attention_weights)\n",
        "    \n",
        "    # Apply attention weights\n",
        "    attended = layers.Multiply(name='attention_apply')([inputs, attention_weights])\n",
        "    \n",
        "    # Sum over sequence dimension to get fixed-size representation\n",
        "    attended = layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), name='attention_sum')(attended)\n",
        "    \n",
        "    return attended\n",
        "\n",
        "def build_attention_encoder(vocab_size, embedding_dim=128, lstm_units=128, maxlen=120, attention_dim=128):\n",
        "    \"\"\"\n",
        "    Build Attention-based Encoder model for semantic similarity.\n",
        "    \n",
        "    Architecture:\n",
        "    - Shared Embedding layer\n",
        "    - Shared Bidirectional LSTM (return_sequences=True)\n",
        "    - Self-attention layer\n",
        "    - Combine using [u, v, |u-v|, u*v]\n",
        "    - Dense layers with dropout\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    input1 = layers.Input(shape=(maxlen,), name='input1')\n",
        "    input2 = layers.Input(shape=(maxlen,), name='input2')\n",
        "    \n",
        "    # Shared embedding layer\n",
        "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True, name='embedding')\n",
        "    \n",
        "    # Shared BiLSTM layer (return sequences for attention)\n",
        "    bilstm = layers.Bidirectional(\n",
        "        layers.LSTM(lstm_units, return_sequences=True), \n",
        "        name='bilstm'\n",
        "    )\n",
        "    \n",
        "    # Process both inputs through shared layers\n",
        "    embedded1 = embedding(input1)\n",
        "    embedded2 = embedding(input2)\n",
        "    \n",
        "    # Apply BiLSTM\n",
        "    lstm_out1 = bilstm(embedded1)\n",
        "    lstm_out2 = bilstm(embedded2)\n",
        "    \n",
        "    # Apply attention\n",
        "    attended1 = attention_layer(lstm_out1, attention_dim=attention_dim)\n",
        "    attended2 = attention_layer(lstm_out2, attention_dim=attention_dim)\n",
        "    \n",
        "    # Combine vectors: [u, v, |u-v|, u*v]\n",
        "    diff = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([attended1, attended2])\n",
        "    multiply = layers.Lambda(lambda x: x[0] * x[1])([attended1, attended2])\n",
        "    concat = layers.Concatenate()([attended1, attended2, diff, multiply])\n",
        "    \n",
        "    # Dense layers\n",
        "    dense1 = layers.Dense(128, activation='relu', name='dense1')(concat)\n",
        "    dropout1 = layers.Dropout(0.3, name='dropout1')(dense1)\n",
        "    dense2 = layers.Dense(64, activation='relu', name='dense2')(dropout1)\n",
        "    output = layers.Dense(1, activation='sigmoid', name='output')(dense2)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs=[input1, input2], outputs=output, name='Attention_Encoder')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "print(\"Building Attention-based Encoder model...\")\n",
        "attention_model = build_attention_encoder(\n",
        "    vocab_size, embedding_dim=128, lstm_units=128, maxlen=120, attention_dim=128\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "attention_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "attention_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callbacks for training\n",
        "attention_callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint('attention_model.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n",
        "]\n",
        "\n",
        "# Train model\n",
        "print(\"Training Attention-based Encoder model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "attention_history = attention_model.fit(\n",
        "    [X1_train, X2_train], y_train,\n",
        "    validation_data=([X1_val, X2_val], y_val),\n",
        "    batch_size=64,\n",
        "    epochs=25,\n",
        "    callbacks=attention_callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "attention_training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {attention_training_time:.2f} seconds\")\n",
        "print(\"Best model weights restored automatically by EarlyStopping\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X1_test, X2_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model and return metrics.\n",
        "    \"\"\"\n",
        "    # Predict probabilities and labels\n",
        "    y_pred_proba = model.predict([X1_test, X2_test], batch_size=64, verbose=0)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    \n",
        "    # PR curve\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'precision_curve': precision_curve,\n",
        "        'recall_curve': recall_curve,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"Evaluating Siamese BiLSTM model...\")\n",
        "bilstm_results = evaluate_model(bilstm_model, X1_test, X2_test, y_test, \"Siamese BiLSTM\")\n",
        "\n",
        "print(\"Evaluating Attention-based Encoder model...\")\n",
        "attention_results = evaluate_model(attention_model, X1_test, X2_test, y_test, \"Attention Encoder\")\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': [bilstm_results['model_name'], attention_results['model_name']],\n",
        "    'Accuracy': [bilstm_results['accuracy'], attention_results['accuracy']],\n",
        "    'Precision': [bilstm_results['precision'], attention_results['precision']],\n",
        "    'Recall': [bilstm_results['recall'], attention_results['recall']],\n",
        "    'F1-Score': [bilstm_results['f1_score'], attention_results['f1_score']],\n",
        "    'ROC-AUC': [bilstm_results['roc_auc'], attention_results['roc_auc']]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves for both models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(bilstm_history.history['loss'], label='BiLSTM Train Loss', linewidth=2)\n",
        "axes[0, 0].plot(bilstm_history.history['val_loss'], label='BiLSTM Val Loss', linewidth=2)\n",
        "axes[0, 0].plot(attention_history.history['loss'], label='Attention Train Loss', linewidth=2)\n",
        "axes[0, 0].plot(attention_history.history['val_loss'], label='Attention Val Loss', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training and Validation Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "axes[0, 1].plot(bilstm_history.history['accuracy'], label='BiLSTM Train Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(bilstm_history.history['val_accuracy'], label='BiLSTM Val Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(attention_history.history['accuracy'], label='Attention Train Accuracy', linewidth=2)\n",
        "axes[0, 1].plot(attention_history.history['val_accuracy'], label='Attention Val Accuracy', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Training and Validation Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# ROC curves\n",
        "axes[1, 0].plot(bilstm_results['fpr'], bilstm_results['tpr'], \n",
        "                label=f\"BiLSTM (AUC = {bilstm_results['roc_auc']:.3f})\", linewidth=2)\n",
        "axes[1, 0].plot(attention_results['fpr'], attention_results['tpr'], \n",
        "                label=f\"Attention (AUC = {attention_results['roc_auc']:.3f})\", linewidth=2)\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].set_title('ROC Curves')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# PR curves\n",
        "axes[1, 1].plot(bilstm_results['recall_curve'], bilstm_results['precision_curve'], \n",
        "                label='BiLSTM', linewidth=2)\n",
        "axes[1, 1].plot(attention_results['recall_curve'], attention_results['precision_curve'], \n",
        "                label='Attention', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Recall')\n",
        "axes[1, 1].set_ylabel('Precision')\n",
        "axes[1, 1].set_title('Precision-Recall Curves')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# BiLSTM confusion matrix\n",
        "sns.heatmap(bilstm_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
        "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Siamese BiLSTM - Confusion Matrix')\n",
        "\n",
        "# Attention confusion matrix\n",
        "sns.heatmap(attention_results['confusion_matrix'], annot=True, fmt='d', cmap='Greens', \n",
        "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title('Attention Encoder - Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_predictions(model, X1_test, X2_test, y_test, pairs_df, test_indices, model_name, num_examples=5):\n",
        "    \"\"\"\n",
        "    Analyze correct and incorrect predictions.\n",
        "    test_indices: indices in original pairs_df that correspond to test set\n",
        "    \"\"\"\n",
        "    # Get predictions for all test samples\n",
        "    y_pred_proba = model.predict([X1_test, X2_test], batch_size=64, verbose=0)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "    y_true = y_test\n",
        "    \n",
        "    # Find correct and incorrect predictions\n",
        "    correct_mask = (y_pred == y_true)\n",
        "    incorrect_mask = (y_pred != y_true)\n",
        "    \n",
        "    # Get indices in test set (0 to len(X1_test)-1)\n",
        "    test_correct_indices = np.where(correct_mask)[0][:num_examples]\n",
        "    test_incorrect_indices = np.where(incorrect_mask)[0][:num_examples]\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{model_name} - Correct Predictions\")\n",
        "    print(f\"{'='*80}\")\n",
        "    for test_idx in test_correct_indices:\n",
        "        # Map test index to original pairs_df index\n",
        "        orig_idx = test_indices[test_idx]\n",
        "        text1 = pairs_df.iloc[orig_idx]['text1']\n",
        "        text2 = pairs_df.iloc[orig_idx]['text2']\n",
        "        # Truncate for display\n",
        "        text1_display = text1[:150] + \"...\" if len(text1) > 150 else text1\n",
        "        text2_display = text2[:150] + \"...\" if len(text2) > 150 else text2\n",
        "        true_label = int(y_true[test_idx])\n",
        "        pred_label = int(y_pred[test_idx])\n",
        "        proba = float(y_pred_proba[test_idx][0])\n",
        "        \n",
        "        print(f\"\\nPair {orig_idx} (test idx {test_idx}):\")\n",
        "        print(f\"Text1: {text1_display}\")\n",
        "        print(f\"Text2: {text2_display}\")\n",
        "        print(f\"True Label: {true_label}, Predicted: {pred_label}, Probability: {proba:.3f}\")\n",
        "        if true_label == 1:\n",
        "            print(\"Reason: Semantic similarity correctly identified - both clauses are from same category\")\n",
        "        else:\n",
        "            print(\"Reason: Correctly identified as different - clauses are from different categories\")\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{model_name} - Incorrect Predictions\")\n",
        "    print(f\"{'='*80}\")\n",
        "    for test_idx in test_incorrect_indices:\n",
        "        # Map test index to original pairs_df index\n",
        "        orig_idx = test_indices[test_idx]\n",
        "        text1 = pairs_df.iloc[orig_idx]['text1']\n",
        "        text2 = pairs_df.iloc[orig_idx]['text2']\n",
        "        # Truncate for display\n",
        "        text1_display = text1[:150] + \"...\" if len(text1) > 150 else text1\n",
        "        text2_display = text2[:150] + \"...\" if len(text2) > 150 else text2\n",
        "        true_label = int(y_true[test_idx])\n",
        "        pred_label = int(y_pred[test_idx])\n",
        "        proba = float(y_pred_proba[test_idx][0])\n",
        "        \n",
        "        print(f\"\\nPair {orig_idx} (test idx {test_idx}):\")\n",
        "        print(f\"Text1: {text1_display}\")\n",
        "        print(f\"Text2: {text2_display}\")\n",
        "        print(f\"True Label: {true_label}, Predicted: {pred_label}, Probability: {proba:.3f}\")\n",
        "        if true_label == 1 and pred_label == 0:\n",
        "            print(\"Reason: False Negative - Similar clauses not recognized (possible lexical differences)\")\n",
        "        else:\n",
        "            print(\"Reason: False Positive - Different clauses incorrectly matched (possible semantic overlap)\")\n",
        "\n",
        "# Analyze predictions for both models\n",
        "analyze_predictions(bilstm_model, X1_test, X2_test, y_test, pairs_df, test_indices, \n",
        "                   \"Siamese BiLSTM\", num_examples=5)\n",
        "analyze_predictions(attention_model, X1_test, X2_test, y_test, pairs_df, test_indices, \n",
        "                   \"Attention Encoder\", num_examples=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Observations and Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance Comparison\n",
        "\n",
        "Based on the evaluation metrics:\n",
        "\n",
        "**Which model generalizes better?**\n",
        "- The model with higher validation accuracy and lower validation loss during training typically generalizes better.\n",
        "- The model with higher test set ROC-AUC score shows better ability to distinguish between similar and dissimilar clauses.\n",
        "- F1-score provides a balanced measure considering both precision and recall.\n",
        "\n",
        "**Which is faster?**\n",
        "- Training time comparison will be shown in the final metrics section below.\n",
        "- Inference time can be measured, but generally BiLSTM is faster due to simpler architecture.\n",
        "\n",
        "**Observations about legal text challenges:**\n",
        "\n",
        "1. **Legal Terminology**: Legal documents use specialized vocabulary and domain-specific terms that require careful semantic understanding.\n",
        "\n",
        "2. **Long Dependencies**: Legal clauses often contain long sentences with complex dependencies that require models to capture long-range context.\n",
        "\n",
        "3. **Subtle Differences**: Many legal clauses may appear similar but have critical differences (e.g., \"shall\" vs \"may\", specific conditions).\n",
        "\n",
        "4. **Formal Language**: Legal text uses formal language patterns that differ from everyday text, making it challenging for models trained on general text.\n",
        "\n",
        "5. **Context Sensitivity**: The same phrase might have different meanings in different legal contexts, requiring deep semantic understanding.\n",
        "\n",
        "6. **Pair Creation Challenges**: Creating balanced positive/negative pairs is crucial - clauses from the same category should be semantically similar, but some categories may have more variation than others.\n",
        "\n",
        "### Model Strengths and Weaknesses\n",
        "\n",
        "**Siamese BiLSTM:**\n",
        "- Strengths: Simpler architecture, faster training, good baseline performance\n",
        "- Weaknesses: May struggle with long sequences, less attention to important words\n",
        "\n",
        "**Attention-based Encoder:**\n",
        "- Strengths: Can focus on important parts of the text, better handling of long sequences\n",
        "- Weaknesses: More complex, potentially slower, may overfit with limited data\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "1. **Data Augmentation**: Consider generating more training pairs, especially for underrepresented clause types.\n",
        "\n",
        "2. **Hyperparameter Tuning**: Experiment with different embedding dimensions, LSTM units, and learning rates.\n",
        "\n",
        "3. **Ensemble Methods**: Combining both models might improve performance.\n",
        "\n",
        "4. **Domain-Specific Embeddings**: While we used learned embeddings, domain-specific pre-training could help.\n",
        "\n",
        "5. **Evaluation Metrics**: Consider additional metrics like MRR (Mean Reciprocal Rank) for ranking tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print final metrics table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL METRICS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nTraining Times:\")\n",
        "print(f\"BiLSTM: {bilstm_training_time:.2f} seconds\")\n",
        "print(f\"Attention: {attention_training_time:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nModels saved:\")\n",
        "print(f\"- bilstm_model.h5\")\n",
        "print(f\"- attention_model.h5\")\n",
        "print(f\"- tokenizer.json\")\n",
        "print(f\"- pairs.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
